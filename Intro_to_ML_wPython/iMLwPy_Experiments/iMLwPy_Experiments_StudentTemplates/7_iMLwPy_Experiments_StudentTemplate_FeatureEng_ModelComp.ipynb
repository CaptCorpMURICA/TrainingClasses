{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Template Experiments 7: Feature Engineering and Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import mglearn as mglearn\n",
    "import mglearn.plots\n",
    "import mglearn.datasets\n",
    "import matplotlib.pyplot as plt\n",
    "# ensures plots are inlined for the notebook presentation\n",
    "%matplotlib inline \n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "import time\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci-Kit Imports\n",
    "import sklearn as sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.linear_model import Lasso\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.decomposition import NMF\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.cluster import AgglomerativeClustering\n",
    "# from sklearn.cluster import DBSCAN\n",
    "\n",
    "# import sklearn.datasets as datasets\n",
    "# from sklearn.datasets import make_blobs\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# from sklearn.datasets import fetch_lfw_people\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.datasets import make_moons\n",
    "# from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# warnings.simplefilter(action='ignore', category=WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Wine data and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wine data set, and describe\n",
    "# from sklearn.datasets import load_wine\n",
    "# wine = load_wine(return_X_y=False)\n",
    "\n",
    "# print(wine['DESCR'])\n",
    "\n",
    "# review key, value\n",
    "# for key,value in wine.items():\n",
    "#     print(key,'\\n',value,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Target as separated by distinct alcohol distributions showing three relatively\n",
    "# separate classes for wine df using sns distplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Correlation Heatmap\n",
    "\n",
    "\n",
    "# NOTE: total_phenols and flavanoids show a high correlation appearing to be above or about 0.8 \n",
    "#       on our correlation heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with no feature selection to remove unnecessary variables\n",
    "# Create Train Test Split\n",
    "\n",
    "\n",
    "# fit X_train and y_train\n",
    "\n",
    "\n",
    "# score y and print\n",
    "\n",
    "\n",
    "# NOTE: Basic RFC has a ~93% accuracy score (your results may vary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection (drop feature) with multicollinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping highly correlated total_phenols (vs. flavanoids) for optimized analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model with feature selection\n",
    "# X = Remove y from wine_df (so we can use 'engineered' dataframe)\n",
    "\n",
    "\n",
    "# y = Remove X from wine_df by selecting the Target column\n",
    "\n",
    "\n",
    "\n",
    "# Train Test Split\n",
    "\n",
    "\n",
    "# score accuracy\n",
    "\n",
    "\n",
    "# NOTE: Random Forest model 1 with Feature Selection to remove multicollinearity improves score to 96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction via PCA on Random Forest model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA as Feature Extraction as reduced feature may not be identical to any original feature\n",
    "# Run RandomForestClassifier 'as is'\n",
    "# make pipeline with PCA and RandomForestClassifier\n",
    "\n",
    "\n",
    "# accuracy score\n",
    "\n",
    "\n",
    "# score PCA with R^2 .score\n",
    "\n",
    "\n",
    "# NOTE: Prediction accuracy with PCA on a few less than 13 components remains the same as base model \n",
    "#      since the RFC also reduces multicollinearity as part of its process and accuracy remains ~96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest model 2 with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RandomForestClassifier with additional hyperparameter tuning\n",
    "\n",
    "\n",
    "# Fit to training data for X and y\n",
    "\n",
    "# Predict y by applying fitted RFC to X test features\n",
    "\n",
    "\n",
    "# Score y test holdout against y predictions\n",
    "\n",
    "\n",
    "# Score train and tests for comparison as well\n",
    "\n",
    "\n",
    "# NOTE: Prediction accuracy improves with additional hyperparameter tuning on RFC model\n",
    "#       increasing prediction accuracy to ~98.15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest model 3 - with addt'l hyperparameter tuning examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#///////////////// Multiple RFC's with Tuning, but no improvemnet //////////////////////\n",
    "# Run RandomForestClassifier with additional hyperparameter tuning ~96%\n",
    "# rf3 = RandomForestClassifier(criterion='gini', n_estimators=288, n_jobs=2, \n",
    "#                             random_state=321, max_depth=5, bootstrap=True)\n",
    "\n",
    "# Run RandomForestClassifier with additional hyperparameter tuning ~96%\n",
    "# rf4 = RandomForestClassifier(criterion='entropy', n_estimators=288, n_jobs=2, \n",
    "#                             random_state=321, max_depth=5, bootstrap=True, verbose=2)\n",
    "\n",
    "# Run RandomForestClassifier with additional hyperparameter tuning ~94%\n",
    "# rf5 = RandomForestClassifier (bootstrap=True, class_weight=None, criterion='gini',\n",
    "#             max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "#             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#             min_samples_leaf=1, min_samples_split=2,\n",
    "#             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "#             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "#///////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "\n",
    "# Simplest model but no more improvement over ~98.15%\n",
    "\n",
    "# Create Random Forest model\n",
    "\n",
    "# Fit to training data for X and y\n",
    "\n",
    "\n",
    "# Predict y by applying fitted RFC to X test features\n",
    "\n",
    "\n",
    "# Score y test holdout against y predictions\n",
    "\n",
    "\n",
    "# Score train and tests for additional evaluation\n",
    "\n",
    "\n",
    "# NOTE: Prediction accuracy does not improve with addit'l hyperparameter tuning on RFC model\n",
    "#      increasing prediction accuracy slightly to ~98.15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Kfold and StratifiedKFold Cross-Validations to RF model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library.method import class\n",
    " # method: model_selection, class: StratifiedKFold\n",
    "# Stratified Kfold - if the estimator is a classifier and y either binary or multiclass,\n",
    "# :class:`StratifiedKFold` is used.\n",
    "\n",
    "# Comparing StratifiedKFold classes\n",
    "# Stratified Kfold 5 Splits\n",
    "\n",
    "\n",
    "# Stratified Kfold 10 Splits\n",
    "\n",
    "\n",
    "# Stratified Kfold 5 Splits, shuffle, random state\n",
    "\n",
    "\n",
    "# NOTE: K-fold with 5 splits, shuffle, random applied to RF model 6 appears to have the best\n",
    "#       performance without overfitting as with 10 splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Grid Search for-loop method to Wine data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply simple Grid Search to find the best parameters for the Wine data set \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "        # for each combination of parameters, train a RandomForest\n",
    "\n",
    "        # fit to the training data\n",
    "\n",
    "        # evaluate the RFC on the test set)\n",
    "\n",
    "        # if we got a better score, store the score and parameters\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: Model with Grid Search shows signs of overfitting. Will rebuild on combo train+valid and\n",
    "#       evaluate on Test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild Model on Combo Train+Valid and Eval on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train + validation set and test set\n",
    "\n",
    "# split the train + validation set into training and validation set\n",
    "\n",
    "        # for each combination of parameters, train a RandomForest\n",
    "\n",
    "        # fit to the training data\n",
    "\n",
    "        # evaluate the RFC model on the validation set)\n",
    "\n",
    "        # if we got a better score, store the score and parameters\n",
    "\n",
    "\n",
    "# rebuild the model on the combined training and validation set,\n",
    "# and evalaute it on the test set\n",
    "\n",
    "\n",
    "# NOTE: The Best score on the validcation set is 100% and is the same as simple grid search\n",
    "#       above indicating simple grid search was overfitting. Test set score then shows 98% so we\n",
    "#       can likely only claim to classify new data at 98% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
