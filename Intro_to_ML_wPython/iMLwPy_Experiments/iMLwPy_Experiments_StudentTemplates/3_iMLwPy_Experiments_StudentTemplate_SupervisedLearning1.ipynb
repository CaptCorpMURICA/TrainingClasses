{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Template Experiments 3: Supervised Learning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a supervised model on California Housing Data Set\n",
    "Predict the House Value. Use Median House Value as your dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mglearn as mglearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import sklearn as sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch California Housing dataset\n",
    "\n",
    "\n",
    "# Create Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Metadata Descriptives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create colorbar plot with title, xlabel, and ylabel for ca_housing (MedInc vs. MedValue)\n",
    "# plt.figure(figsize=(6,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation - Issues resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create several sns.boxplot and sns.distplot on Median Income, Median Value, House Age review outliers \n",
    "# from exploration histograms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Outliers in MedValue (hint use \">\" greater-than symbol and .value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate MedValue greater than 500K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing removal of MedValue Outlier use sns.distplot (or other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Outliers in MedInc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate MedInc less than 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing removal of MedInc Outlier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Outliers in HouseAge \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate House Age less than 52\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing removal of HouseAge Outlier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Correlation Heatmap using .corr() and sns.heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping highly correlated AveBedrms (vs. AveRooms) for optimized Regression analysis using .drop \n",
    "# (don't forget axis=1 and inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Supervised Model 1: K-Nearest Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics importe mean_squared_error, r2_score\n",
    "\n",
    "# X = Remove y from cal_housing_pd (so we can use 'engineered' dataframe) using .drop for y\n",
    "\n",
    "\n",
    "# y = Remove X from cal_housing_pd by selecting the MedValue column\n",
    "\n",
    "\n",
    "# Train Test Split with a DataFrame (using 'engineered' dataframe)\n",
    "\n",
    "\n",
    "# Transform pd dataframe into array (.values) - required for looping through with the next algorithm\n",
    "# X train, X test, y train, y test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through (leaving the loop as we didn't cover this prior)\n",
    "# knn_results = []\n",
    "# for i in range(1,20):\n",
    "#     # make predictions using looped neighbors\n",
    "#     reg = KNeighborsRegressor(n_neighbors=i)\n",
    "#     reg.fit(X_train, y_train)\n",
    "#     knn_results.append((i, reg.score(X_train, y_train), reg.score(X_test, y_test))) #saves value of n used as a tuple with R2\n",
    "# print(\"Train set predictions:\\n\", reg.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame with resutls\n",
    "# knn_results_df = pd.DataFrame(knn_results, columns=['n','RsqTrain', 'RsqTest'])\n",
    "# knn_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display inflection point of the model where 6 appears to be the best fit on training set\n",
    "# training set n decreases as number of neighbors increases, test set gets more stable at about 6\n",
    "# knn_results_df.plot(x='n', y=['RsqTrain', 'RsqTest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with best fit hyperparameter and then score the model (it happened to be 6 when I ran it but could be \n",
    "# different for yours)\n",
    "\n",
    "# Score and print training\n",
    "\n",
    "\n",
    "# Score and print testing \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Supervised Model 2: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a ridge regression model \n",
    "# from sklearn.linear_model import Ridge #uses regularization on the inputs, calculating the L2 norms (square of the difference)\n",
    "# ridge = Ridge().fit(X_train, y_train) #uses alpha as parameter in the fit\n",
    "\n",
    "# Score and print training with R^2 .score\n",
    "\n",
    "\n",
    "# Score and print testing with R^2 .score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions using looped alpha\n",
    "# ridge_results = []  #leaving the created function for for-loop in as we did not cover prior\n",
    "\n",
    "# for alpha_val in np.linspace(0.0, 1, num=10): #linspace is number of samples between start and end for alpha between 0 and 1\n",
    "#     ridge_regressor = Ridge(alpha=alpha_val)\n",
    "#     ridge_regressor.fit(X_train, y_train)\n",
    "#     ridge_results.append((alpha_val, ridge_regressor.score(X_train, y_train),ridge_regressor.score(X_test, y_test)))\n",
    "# print(\"Train set predictions:\\n\", ridge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datafram with results as we did above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha RsqTest of 0.666667 is highest at 0.567466, so this is the \"best\" model of the hyperparameter tested, \n",
    "# however it doesn't look like that moved the needle at all. (note that your results may vary)\n",
    "\n",
    "# plot results with \"your dataframe\" .plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with best fit results from your ridge regression\n",
    "\n",
    "# Assign alpha\n",
    "\n",
    "\n",
    "# fit the model\n",
    "\n",
    "\n",
    "# Score and print training with R^2 .score\n",
    "\n",
    "\n",
    "# Score and print testing with R^2 .score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Supervised Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict median house value using RF Regression.\n",
    "# Will not need to resample data, will re-use X_train and y_train datasets as re-sample between algorithm runs would be changing\n",
    "# the input data and then we won't be able to tell which improved the fit\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor and assign it\n",
    "\n",
    "\n",
    "# fit X_train and y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just using the defaults, it provides a model fit of 0.96. Which when we see a score\n",
    "# this high, it almost certainly means the model is overfit. When we score the test set, we see a fit of 0.76,\n",
    "# which is quite a bit of deterioration compared to the training set. So we should try to set of inputs,\n",
    "# such that the performance between the training and test sets are possibly improved. \n",
    "\n",
    "\n",
    "# Score and print training with R^2 .score\n",
    "\n",
    "\n",
    "# Score and print testing with R^2 .score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we suspect the model is overfitting, it suggests that modifying hyperparameters such as \n",
    "# max_depth (the number of variable interactions) and min_samples_leaf (the number of data points \n",
    "# in any given segment of data) may help. So requiring a minimum number of data points in a segment, \n",
    "# we likely get a more generalized model, instead of the current default of a minimum of 1 value in each leaf\n",
    "\n",
    "# Vary min_samples_leaf with loop through \n",
    "# leaving in the for loop as we di dnot cover this prior\n",
    "# rf_results = []\n",
    "# for i in range(1, 20):\n",
    "#     # make predictions using looped neighbors\n",
    "#     regr_loop = RandomForestRegressor(min_samples_leaf=i, random_state=321)\n",
    "#     regr_loop.fit(X_train, y_train)\n",
    "#     rf_results.append((i, regr_loop.score(X_train, y_train), regr_loop.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results in a dataframe\n",
    "# rf_results_df = pd.DataFrame(rf_results, columns=['min_samples_leaf','RsqTrain', 'RsqTest'])\n",
    "# rf_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples_leaf RsqTest of 5 is highest at 0.773620, so this is the \"best\" model of the hyperparameters we tested. \n",
    "# With larger datasets we would set the min_values_leaf to several hundred, or even 1% of the overall sample.\n",
    "# note your results may vary\n",
    "\n",
    "# show results in a .plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess and Select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing KNN with 6 neighbors as best fit \n",
    "# (remember 6 was the optimal parameter value from our experiment above, however yours may vary)\n",
    "\n",
    "# Assign the hyper parameter value of neighgors\n",
    "\n",
    "\n",
    "# fit the model for X_train and y_train\n",
    "\n",
    "\n",
    "# Score and print training with R^2 .score\n",
    "\n",
    "\n",
    "# Score and print testing with R^2 .score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing Ridge Regressor with alpha of .666667 as best fit\n",
    "# (remember .666667 was the optimal parameter value from our experiment above, however yours may vary)\n",
    "\n",
    "# Assign the hyper parameter value of neighgors\n",
    "\n",
    "# fit the model for X_train and y_train\n",
    "\n",
    "\n",
    "# Score and print training with R^2 .score\n",
    "\n",
    "\n",
    "# Score and print testing with R^2 .score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing Random Forest Regression with min_samples_leaf of 5\n",
    "# (remember 5 was the optimal parameter value from our experiment above, however yours may vary)\n",
    "\n",
    "# Assign the hyper parameter value of neighgors\n",
    "\n",
    "\n",
    "# fit the model for X_train and y_train\n",
    "\n",
    "\n",
    "# Score and print training with R^2 .score\n",
    "\n",
    "\n",
    "# Score and print testing with R^2 .score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Example Conclusions\n",
    "* KNN shows the more believable model than Ridge Regressor as there is variance between the training and test set. Pros:  Doesn’t require any assumptions about the data or its distributions (i.e. it’s “nonparametric”), which means it is not sensitive to high correlation between data features. Cons: Because the optimization metric is “closeness”, it does depend on the amount of data we have, and our 20k sample may not be enough to reach parity.\n",
    "\n",
    "* Tried Ridge Regressor because it has regularization, but realize that this is not the best model type to fit the data set. This is why alpha of .66667 was showing the 'best' score but negligibly moved the needle, because the coefficients were being barely restricted. Pros: Ridge regression is a parametric methods and assumes something about the data distribution, such as having normally distributed residuals. Ridge regression normalizes the model coefficients, which helps in scenarios where you do have high-correlations between data features. Cons: In the case of our CA Housing data, there were no high-correlations, non-linear data.\n",
    "\n",
    "* Random Forest has the best fit model of all three and is more believeable with the tuned hyperparameter of min_samples_leaf. The Training set and Test set scores have a believeable variance and the test set score is lower than the training fit as it should be. Pros: Random Forest uses lots of different models as set by the n_estimator hyperparameter. By sampling some of the rows, and some of the features for each model, we can model lots of different subsets of the data, finding additional information that we don’t get by picking a single “best” model. Cons: Because we can fit huge numbers of models, random forest can take a long time to calculate and is more prone to overfitting such as we saw with the default values. However, as the hyperparameter of min_samples_leaf tuned the model, we saw a believeable improvement and should pick Random Forest as our best fit model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
