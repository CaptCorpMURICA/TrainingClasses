{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Template Experiments 4: Supervised Learning 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import mglearn as mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "# ensures plots are inlined for the notebook presentation\n",
    "%matplotlib inline \n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import time\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci-Kit Imports\n",
    "import sklearn as sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.datasets import make_blobs\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# from sklearn.datasets import make_moons\n",
    "# from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# warnings.simplefilter(action='ignore', category=WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn Wine Data Set - Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Wine and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Wine dataset\n",
    "\n",
    "\n",
    "# print Metadata Descriptives keys, features, shape, type of target, target, shape of target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas DataFrame\n",
    "\n",
    "# wine_pd.nlargest(25, 'alcohol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics with describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histograms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier Model - ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting, roc, binarize and decision tree tools\n",
    "# from itertools import cycle\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# reload wine for classification\n",
    "\n",
    "\n",
    "# Binarize target\n",
    "\n",
    "\n",
    "\n",
    "# train test spilt\n",
    "\n",
    "\n",
    "\n",
    "# Classification - create decision tree classifier with multiclass import\n",
    "# dt_clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=321))\n",
    "# wine_target_score = dt_clf.fit(X_train, y_train).predict_proba(X_test)\n",
    "\n",
    "# create dictionary elements\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "\n",
    "## Leaving this all in as we did not cover loops in the book\n",
    "\n",
    "# create for loops for classes and colors of ROC curve plot\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test[:, i], wine_target_score[:, i])\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "# colors = cycle(['blue', 'red', 'green'])\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, lw=2.0,\n",
    "#              label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "#              ''.format(i, roc_auc[i]))\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "         \n",
    "# Plot of a ROC curve for each class\n",
    "# for i in range(n_classes):\n",
    "#     plt.figure()\n",
    "#     plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "#     plt.plot([0, 1], [0, 1], 'k--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title('Receiver operating characteristic')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier Model - ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports done in decision tree classifier above\n",
    "# reload for classification done in decision tree classifier above\n",
    "# Binarize target decision tree classifier above\n",
    "# train test spilt decision tree classifier above\n",
    "\n",
    "# Repeat same as above for ROC/AUC but with random forest classifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Classification for Random Forest\n",
    "\n",
    "# create dictionaries\n",
    "\n",
    "# create for loops for classes and colors\n",
    "\n",
    "         \n",
    "# Plot of a ROC curve for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Decision Tree Classifier Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat data for scalability with \"pipelined scaling\" as seen in histograms showing heterogeneous \n",
    "# scale due to differing properties of each component\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# binarize wine target - leaving in a bit of the code for an example\n",
    "# wine_target_binarize = label_binarize(wine_target, classes=[0,1,2])\n",
    "# n_classes = 3\n",
    "\n",
    "# load wine and create X and y, then train, test split\n",
    "\n",
    "\n",
    "# Fit to data and predict using pipelined scaling, PCA, and DecisionTreeClassifier\n",
    "\n",
    "# print accuracy score on y test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix on (y_test, pred_test_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification report for (y_test, pred_test_dt)\n",
    "\n",
    "\n",
    "# Decision Tree Classifier with standardization via scalaing gives the precision accuracy of ~93%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-propagation Neural Network - Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kpfeif200\\AppData\\Local\\Continuum\\anaconda3\\envs\\python_clean\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# This model didn't work for me at the end as the original tensorflow activty was writting on a different data set.\n",
    "# This will need to be recoded in the bottom cell to address that this set does not have test and train split out as nmist does\n",
    "# leaving the code in here as it is too complicated to comment out in a way that could be templatized. \n",
    "# Here is a place with documentation that should help with the correction needed: \n",
    "#           https://www.tensorflow.org/api_docs/python/tf/train/batch\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# load ca housing\n",
    "ca_housing = fetch_california_housing(\n",
    "    data_home=None, download_if_missing=True, return_X_y=False)\n",
    "\n",
    "# train test and split ca housing\n",
    "X_train, X_test, y_train, y_test = train_test_split(ca_housing.data, ca_housing.target, \n",
    "                                                    test_size=0.30, random_state=321)\n",
    "# create .train and .test\n",
    "ca_housing.train = (X_train, y_train)\n",
    "ca_housing.test = (X_test, y_test)\n",
    "\n",
    "learning_rate = 0.1\n",
    "# epochs = 10\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "\n",
    "# Network parameters\n",
    "n_hidden_1 = 10 # 1st layer number of neurons\n",
    "n_hidden_2 = 10 # 2nd layer of neurons\n",
    "num_input = 20640 # ca_housing data input\n",
    "num_classes = 2 # ca_housing total classes (2 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "        'h1':\n",
    "            tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "        'h2':\n",
    "            tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out':\n",
    "            tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "            'b1':\n",
    "                tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "            'b2':\n",
    "                tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "            'out':\n",
    "                tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-52d1594556df>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "# Hidden fully connected layer with 10 neurons\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "# Hidden fully connected layer with 10 neurons\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "# Output fully connected layer with a neuron for each class\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "    \n",
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "#Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'next_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7be158cfca3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mca_housing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;31m# Run optimization op (backprop)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'next_batch'"
     ]
    }
   ],
   "source": [
    "#Start training with ca_housing\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = ca_housing.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X:batch_x, Y:batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            #Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y:batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.3f}\".format(acc))\n",
    "    print(\"Optimization Finished\")   \n",
    "    \n",
    "    # Calculate accuracy for ca_housing\n",
    "    print(\"Testing Accuracy: \", accuracy.eval(\n",
    "        {X: ca_housing.test.data, Y: ca_housing.test.target}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
